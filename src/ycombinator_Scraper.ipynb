{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcUseh_yUomD"
   },
   "source": [
    "In this notebook we scrape list of ycombinator funded companies as well as their information (e.g location, team size, description, etc.), and then save them in a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Xp36ghVV0f"
   },
   "source": [
    "# URL of Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p53-1FwAVbwK"
   },
   "source": [
    "In this part we check available ids for ycombinatior's companies and save their urls to scrape their information in the next section.\n",
    "\n",
    "Indeed we can have access to each company's page on the ycombinator website by having id of each company and using the \"http://ycombinator.com/companies/{id}\" url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nHge_YjVVLK8"
   },
   "outputs": [],
   "source": [
    "# Importing all necessery packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Id interval that we check their availabilities on the ycombinator website\n",
    "START_ID=1\n",
    "END_ID = 10\n",
    "ULRS_PATH = \"working_urls_{}-{}.csv\".format(str(START_ID), str(END_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_hsJEz_fb4dy"
   },
   "outputs": [],
   "source": [
    "cheking_ulrs = [\"https://www.ycombinator.com/companies/\" + str(id) for id in range(START_ID,END_ID+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntKRkc6EfFZe",
    "outputId": "08eebae3-c23a-4d53-aee4-0e0210cd1b54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:00<02:02,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/500 [00:00<02:32,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 4/500 [00:01<02:50,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter2 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 5/500 [00:01<03:04,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter3 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61/500 [00:26<02:43,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter59 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 65/500 [00:28<03:02,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter63 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 69/500 [00:30<03:16,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter67 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 114/500 [00:50<03:31,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter112 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 238/500 [01:36<01:42,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter236 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 382/500 [02:32<00:45,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter380 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 383/500 [02:33<00:46,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter381 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 386/500 [02:34<00:33,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter384 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 437/500 [02:55<00:33,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter435 - HTTPError: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:20<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#check if each url is valid and the http request returns 200 code\n",
    "working_urls=[]\n",
    "with tqdm(total = len(cheking_ulrs)) as pbar:\n",
    "    for i, url in enumerate(cheking_ulrs):\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            _ = urllib.request.urlopen(url)\n",
    "        except urllib.error.HTTPError as e:\n",
    "          # Return code error \n",
    "          print('iter{} - HTTPError: {}'.format(i, e.code))\n",
    "        except urllib.error.URLError as e:\n",
    "          # Url error\n",
    "          print('iter{} - URLError: {}'.format(i, e.reason))\n",
    "        else:\n",
    "          # url is ok\n",
    "          working_urls.append(url)\n",
    "df_working_urls = pd.DataFrame(working_urls)\n",
    "#since this process is time consuming we save its result\n",
    "df_working_urls.to_csv(ULRS_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJCqO5CAhghn"
   },
   "source": [
    "# Scrape\n",
    "\n",
    "Now that we have urls of companies on the ycombinator, we scrape their page and extract interesting information of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZHio9lThPUc",
    "outputId": "2d4f5ce4-87f0-41b9-9e6f-0f45247f878d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF COMPANIES:  6\n"
     ]
    }
   ],
   "source": [
    "def get_urls(path = ULRS_PATH ):\n",
    "    temp_urls = pd.read_csv(path)\n",
    "    temp_urls.columns = [\"index\", \"links\"]\n",
    "    urls = temp_urls[\"links\"].values.tolist()\n",
    "    return urls\n",
    "urls = get_urls()\n",
    "print(\"NUMBER OF COMPANIES: \", len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cuIHsdWkLY5",
    "outputId": "e1b71b79-d7f7-4e48-d91f-d1afac445024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:08<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_titles = [\"NAME\",\n",
    "              \"FOUNDATION YEAR\",\n",
    "              \"SIZE\",\n",
    "              \"LOCATION\",\n",
    "              \"BATCH\",\n",
    "              \"SHORT DESC\",\n",
    "              \"LONG DESC\",\n",
    "              ]\n",
    "\n",
    "result = pd.DataFrame(columns=csv_titles)\n",
    "data=[]\n",
    "with tqdm(total = len(urls)) as pbar:\n",
    "    for i in range(len(urls)):\n",
    "        url = urls[i]\n",
    "        request = requests.get(url)\n",
    "        text = request.text\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "        # we refer to html file of one of companies to find class name of each part\n",
    "        # by reffering we found that \n",
    "        #  #company's name is mentioned by h1 tag and class='heavy'\n",
    "        #  #short description is mentioned by h3 tag\n",
    "        #  #long description is mentioned by p tag\n",
    "\n",
    "\n",
    "        name = (soup.find(\"div\", {\"h1\", \"heavy\"})).get_text(strip=True)\n",
    "        short_description = (soup.find(\"h3\").get_text(strip=True))\n",
    "        long_description = (soup.find(\"p\").get_text(strip=True))\n",
    "\n",
    "        #The other information are under class=\"highlight-box\"\n",
    "        details = (soup.find((\"div\"), (\"highlight-box\"))).get_text(strip=True)\n",
    "        #Here we extract information from details using regular expression since all companies'\n",
    "        # detail follow this expression styly\n",
    "        # e.g CircuitHubFounded2012Team Size30LocationLondon, United Kingdom\n",
    "        match_obj = re.match(r'(.*)Founded(\\d*)Team Size(\\d*)Location(.*)', details, re.M | re.I)\n",
    "        \n",
    "        year = match_obj.group(2)\n",
    "        size = match_obj.group(3)\n",
    "        location = match_obj.group(4)\n",
    "        # batch is mentioned by class = pill pill-orange\n",
    "        batch = (soup.find((\"span\"), (\"pill pill-orange\"))).get_text(strip=True)\n",
    "        dict = {\n",
    "            \"NAME\": name,\n",
    "            \"FOUNDATION YEAR\": year,\n",
    "            \"SIZE\": size,\n",
    "            \"LOCATION\": location,\n",
    "            \"BATCH\": batch,\n",
    "            \"SHORT DESC\": short_description,\n",
    "            \"LONG DESC\": long_description,\n",
    "            }\n",
    "        data.append(dict)\n",
    "        pbar.update(1)\n",
    "#save result\n",
    "result= result.append(pd.DataFrame.from_records(data, columns=dict.keys()))\n",
    "result.to_csv(\"yclist_{}.csv\".format(str(len(urls))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ycombinator Scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
